---
title: 用Scrapy创建可管理、多站点的爬虫实战
date: 2020-02-04 16:07:03
comments: true
categories:
	- Programming
tags:
	- spider
	- scrapy
	- Python
	- article
---

本文介绍，如何从零开始，一步一步的，利用Python及Scrapy创建一个可以抓取不同站点，可管理的爬虫。

# 背景

小明（小明你好）喜欢阅读网文，但网文往往很多广告不易阅读，并且他没有很多移动流量，
就想着在 WIFI 下载好了然后可以不用联网一直看，于是让我们来分析他的需求：

## 原始需求

* 因为不同的网文在不同的网站上，所以要能够从不同的网站上下载
* 能够搜索一本小说，并下载这本小说的所有章节，而这本小说下载后，能自动更新
* 下载的章节要能优化显示不要乱糟糟

## 需求分析

		年纪大了，懒得闲扯了，直接来具体的。。。


* 能爬不同小说站
  * 支持的小说站定义
* 能爬全站分类，小说，目录，每章 4 级页面
  * 每个站对应要定义这几级的规则
* 能搜索指定小说（两个方案，一个是网站自身的搜索，另外是百度搜素。百度搜索同时适合网站不提供搜索的情况）
  * 能解析搜索返回页面
* 能列出队列中的所有小说
* 能选择任一队列中的小说开始下载
* 可以指定某下载小说从N~M章节（不一定和页面能对上）开始下载
* 净化小说页面
  * 页面规则抽取
  * 使用净化库
* 设定能使用不同的间隔时间
* 设定能使用不同的IP（IP池）
* 当规则发生变化后，不再下载该网站，并报出问题


# 设计

从用例开始，综合考虑选型、数据库设计和架构设计。
因为已确定使用scrapy，所以以scrapy流程为基础来设计。

## 用例

*以下所说**开关触发**是指函数、命令行命令、数据库数据等各种方式，
具体看实现*

#### 爬取网站索引

使用一个开关触发爬虫，爬虫循环所有支持的网站，爬取其所有索引页，
将索引页上的小说及地址解析并存储

此开关支持参数：

* 无参数：爬取所有支持的网站
* 参数 `site_id`：爬取指定网站


#### 爬取小说目录（及信息）

使用一个开关触发爬虫，爬取已经存储的小说地址，从而获得小说目录（及信息），
并存储到数据库

此开关支持参数：

* 无参数：爬取所有已存储的小说信息
* 参数 `site_id`：爬取指定网站的所有已存储小说章节（及信息）
* 参数 `novel_id`：爬取指定的小说章节（及信息）

#### 爬取小说章节

使用一个开关触发爬虫，爬取已经存储的小说章节，解析内容并存储。
如果小说章节已经爬取过，则继续爬取后面的章节

此开关支持参数：

* 无参数：爬取所有已存储的小说的所有章节
* 参数 `site_id`：爬取指定网站的所有小说章节
* 参数 `novel_id`：爬取指定小说的所有章节
* 参数 `chapter_from_id`：从指定章节id开始
* 参数 `chapter_end_id`：爬取到指定章节id结束

#### 搜索小说

使用一个开关触发，搜索指定的小说，解析搜索结果页面，存储前N条结果
可选取其中某条为指定小说爬取
每次新的搜索清除前次结果

参数：
* 参数 `novel_name`：小说名
* 参数 `list`：列出所有返回值
* 参数 `clean`： 清除搜索结果
* 参数 `select_id`：选的结果id，该小说被加入要爬取队列


## 数据（数据库，缓存，文件等）

* 所有支持的网站的定义
* 记录每个网站的小说索引
* 记录所有的小说
* 记录一个小说所有章节
* 爬取冲突/重复的记录
* 分布式爬取时的锁
* 所有的小说封面图片

#### 

## 流程（基于scrapy）

```flow
st=>start: Start:>http://www.google.com[blank]
e=>end:>http://www.google.com
op1=>operation: My Operation
sub1=>subroutine: My Subroutine
cond=>condition: Yes
or No?:>http://www.google.com
io=>inputoutput: catch something...
para=>parallel: parallel tasks

st->op1->cond
cond(yes)->io->e
cond(no)->para
para(path1, bottom)->sub1(right)->op1
para(path2, top)->op1
```

### Spider 

* 爬取网站首页获得索引页面的 Home Spider
* 爬取索引页面获得小说列表的 Index Spider
* 爬取小说页面和章节的 Novel Spider
* 爬取小说章节的 Chapter Spider

